%Extreme-scale computing presents some unique challenges to fault tolerance as faults are no longer 
%an exceptional event \cite{ferreira_sc_2011}. 

The scale of HPC 

Rollback and recovery are predominate mechanisms to achieve fault
tolerance in current HPC environments. In the most general form, the rollback and recovery mechanism
involves the periodic saving of the current system state to stable storage, with the anticipation that
in the case of a system failure, computation can be restarted from the most recently saved state \cite{Elnozahy:02:Survey}. %The identification of an error, before or during a checkpoint,
%requires that the application rollback to the previously completed checkpoint. 
Coordinated
Checkpoint is a method to achieve a globally consistent state. Specifically, all processes
coordinate with one another to produce individual states that satisfy the ``happens before"
communication relationship \cite{chandy_trans_1972}, which is proved to provide a consistent global state.
%Essentially, the algorithm provides a method for all processes involved to stop operation ``at the same
%time" and transfer their system state to a stable storage. 
The major benefit of coordinated
checkpointing stems from its simplicity and ease of implementation. Its major drawback, however, is the
lack of scalability, as it requires global process coordination to achieve a checkpoint 
\cite{elnozahy_dsc_2004,riesen_sandia_2010,hargrove2006berkeley}.


In uncoordinated checkpointing, processes record their states independently and postpone creating a 
globally consistent view until the recovery phase \cite{plank_ftc_1999}. The major advantage is the reduced overhead during fault free operation. However, the scheme requires that
each process maintains multiple checkpoints and message logs, necessary to construct a consistent 
state during recovery. It can also suffer the well-known domino effect, which may result in the 
re-execution of the entire application \cite{randell_domino_effect}. One hybrid approach, known as communication induced 
checkpointing schemes, aims at reducing coordination overhead \cite{alvisi_ftc_1999}. The approach, however, may 
cause processes to store useless states that are never used in future rollbacks. To address this 
shortcoming, ``forced checkpoints" have been proposed \cite{helary_rds_1997}. This approach may lead to unpredictable
checkpointing rates. Although well-explored, uncoordinated checkpointing has not been widely adopted
in HPC environments, due to its dependency on applications \cite{zheng_2004_ftccharm,guermouche_2011_ipdps}.


One of the largest overheads in any checkpointing process is the time necessary to write the checkpointing 
to stable storage, during which the application pause its execution. Incremental checkpointing attempts
to address this shortcoming by only writing the changes since the previous checkpoint \cite{nam_ftc_1997,Agarwal:04:Adaptive}. This
can be achieved using dirty-bit page flags \cite{plank_ftcs_1994,elnozahy_1992_manetho}. Hash based incremental checkpointing, on the other
hand, makes use of hashes to detect changes \cite{nam_ftc_1997,Agarwal:04:Adaptive}. Another proposed scheme, known as copy-on-write,
offloads the checkpointing process to a secondary task and only writes incremental checkpoints \cite{li_trans_1994}.
The main concern of these techniques is that some applications would require increase in their
memory to support the simultaneous execution of the checkpointing and the application. It has been suggested that nodes in extreme-scale systems should be configured with fast local storage, which
improves the performance of checkpointing \cite{doe_ascr_exascale_2011}. Multi-level checkpointing, which consists of
writing checkpoints to multiple storage targets, can benefit from such a strategy \cite{Moody:10:SCR,hakkarine_2013}. This,
however, may lead to increased failure rates of individual nodes and increased per-node cost \cite{chen_2011_hystor}.
Furthermore, it may complicate the checkpoint writing process and requires that the system track the
current location of all process's checkpoints.


Our work is based on process replication, or state machine replication, which has long been used to provide fault tolerance in distributed and mission critical systems\cite{schneider_1990_tutorial,bartlett_1981_nonstop}. Replication can be used to detect and correct system failures that are otherwise undetectable,
such as silent data corruption \cite{ni_2013_acr} and Byzantine faults \cite{fiala_2012_sdc}. Recently, replication is proposed as a
viable alternative to checkpointing in HPC \cite{riesen_sandia_2010,lefray_2013_rsd,Cappello:09:Fault}. 
In addition, full and partial
replication have also been used to augment existing checkpointing techniques, and to guard
against silent data corruption \cite{stearly_2012_partial,elliott_2012_cpr}. There are several different implementations of
replication in the widely used MPI library, each with their different tradeoffs and overheads. The
overhead can be negligible or up to 70\% depending upon the communication patterns of the
application \cite{engelmann2011redundant}. %Moreover, replication alone is not enough to guarantee fault tolerance since
%it is possible that all nodes executing a given process could fail simultaneously, thus
%replication is typically paired with some form of checkpointing. 
The most relevant works to ours is redundant multi-threading (RMT) whereby one leading thread of execution is running ahead of trailing threads \cite{reinhardt2000transient,Wadden:2014:RDE:2665671.2665686}. However, our approach is different in that it tunes the execution rates of the leading and trailing threads in a finer grain, in order to achieve a "parameterized" trade-off between completion time and energy consumption. Further, we take advantage of the recovery time after each failure and "leap" the trailing threads to achieve forward progress, largely improving performance in terms of both completion time and energy consumption. This differs from RMT, of which the "leaping" of the trailing thread results in extra overhead.
To the best of our knowledge,
Lazy Shadowing is the first attempt to explore a state-machine replication based framework
that achieves a fine-grained tradeoff between time and hardware redundancy while meeting resilience and
power requirements.
