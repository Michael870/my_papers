%Computing and information systems have become integral to all aspects of our life, and their significance will inevitably continue to increase in the future. Fueled by powerful computing servers, massive storage capacity and unprecedented communication speeds, these systems support an increasingly large number of services and applications, critical to our society, national security and the economy. These applications range from Internet-scale web services, high-performance scientific computing, data-intensive analytics and graph processing, mobile computing for smart distributed environments, to healthcare and disaster management. As our reliance on IT continues to grow, future infrastructure must support hundreds of thousands, if not millions, of servers, to achieve significantly higher levels of parallelism and handle a massive number of storage and communications operations.

%The scale needed to address our future computing needs will come at the cost of increasingly sophisticated, complex computing and information systems whose behavior is increasingly difficult to specify, predict and manage.
The system scale needed to address our future computing needs will come at the cost of increasing complexity. As a result, the behavior of future computing and information systems will be increasingly difficult to specify, predict and manage.
This upward trend, in terms of scale and complexity, has a direct negative effect on the overall system reliability. 
%The increase in the number of system components significantly increases the propensity of the system to faults, while driving power consumption to unprecedented heights. 
Even with the expected improvement in the reliability of future computing technology, the rate of system level failures will dramatically increase with the number of computing and storage components, possibly by several orders of magnitude. This can be clearly seen in Figure~\ref{fig:exe_failure}. A system with 200,000 nodes is expected to experience hundreds of failures every day, even when the Mean Time Between Failures (MTBF) of an individual node is as large as 5 years. How to achieve fault tolerance for such high level of failures with acceptable overhead is a significant challenge. 

\begin{figure}[!t]
	\begin{center}
		\includegraphics[width=0.7\columnwidth]{Figures/sandia_system_failure_rate_increase_nodes}
	\end{center}
	%\vskip -0.25in 
	\caption{Impact of system size on system resilience.}
	\label{fig:exe_failure}
\end{figure}

Another direct consequence of the increase in system scale is the dramatic increase in power consumption. Recent studies show a steady rise in system power consumption to 1-3MW in 2008, followed by a sharp increase to 10-20MW in subsequent years, with the expectation that power consumption could surpass 50MW by 2016~\cite{doe_ascr_exascale_2011}. The US Department of Energy has recognized this trend and established a power limit of 20MW, challenging the research community to provide a 1000x improvement in performance with only a 10x increase in power~\cite{doe_ascr_exascale_2011}. %It is reported in~\cite{elnozahy2003energy} that energy cost in data centers accounts for 23-50\% of the overall expenses, making system power a leading design constraint in future extreme-scale computing infrastructure.
This huge imbalance makes system power a leading design constraint in future extreme-scale computing infrastructure~\cite{Sarood2014,Villa2014}.


Fault tolerance and power management have been studied extensively, although only recently have researchers begun to study the combination of these two competing goals. In today's systems the response to faults mainly consists of restarting the application, including all related software components that have been affected by the fault. To avoid full re-execution of the original task, systems often checkpoint the execution periodically. Upon the occurrence of a hardware or software failure, recovery is then achieved by restarting the computation from a known checkpoint~\cite{Elnozahy:02:Survey}. 


Coordinated Checkpoint is a method to achieve a globally consistent state. Specifically, all processes
coordinate with one another to produce individual states that satisfy the ``happens before"
communication relationship \cite{chandy_trans_1972}. The major benefit of coordinated
checkpointing stems from its simplicity and ease of implementation. Its major drawback, however, is the
lack of scalability~\cite{elnozahy_dsc_2004,riesen_sandia_2010,hargrove2006berkeley}. In uncoordinated checkpointing, processes record their states independently and postpone creating a 
globally consistent view until the recovery phase \cite{plank_ftc_1999}. The major advantage is the reduced overhead during fault free operation. However, the scheme requires that
each process maintains multiple checkpoints and message logs, necessary to construct a consistent 
state during recovery. It can also suffer the well-known domino effect, which may result in the 
re-execution of the entire application \cite{randell_domino_effect}. Although well-explored, uncoordinated checkpointing has not been widely adopted
in HPC environments, due to its dependency on applications \cite{zheng_2004_ftccharm,guermouche_2011_ipdps}.

One of the largest overheads in any checkpointing process is the time necessary to write the checkpointing 
to stable storage, during which the application pause its execution. Incremental checkpointing attempts
to address this shortcoming by only writing the changes since the previous checkpoint \cite{nam_ftc_1997,Agarwal:04:Adaptive}. This
can be achieved using dirty-bit page flags \cite{plank_ftcs_1994,elnozahy_1992_manetho}. Hash based incremental checkpointing, on the other
hand, makes use of hashes to detect changes \cite{nam_ftc_1997,Agarwal:04:Adaptive}. Another proposed scheme, known as copy-on-write,
offloads the checkpointing process to a secondary task and only writes incremental checkpoints \cite{li_trans_1994}.
The main concern of these techniques is that some applications would require increase in their
memory to support the simultaneous execution of the checkpointing and the application. It has been suggested that nodes in extreme-scale systems should be configured with fast local storage, which
improves the performance of checkpointing \cite{doe_ascr_exascale_2011}. Multi-level checkpointing, which consists of
writing checkpoints to multiple storage targets, can benefit from such a strategy \cite{Moody:10:SCR,hakkarine_2013}. This,
however, may lead to increased failure rates of individual nodes and increased per-node cost \cite{chen_2011_hystor}.
Furthermore, it may complicate the checkpoint writing process and requires that the system track the
current location of all process's checkpoints.


Given the anticipated increase in system-level failure rates and the time required to checkpoint large-scale compute-intensive and data-intensive applications, it is predicted that, in extreme scale computing environments, the time required to periodically checkpoint an application and restart its execution will approach the system's MTBF. Consequently, applications will make little forward progress, thereby reducing considerably the overall performance of the system~\cite{riesen_sandia_2010}.% Given this limitation, several large-scale application designers avoid checkpointing and rely on re-execution, upon failure, to achieve fault-tolerance. 

More recently, process replication, either fully or partially, has been proposed as an alternative to checkpointing, in order to scale to the resilience requirements of large distributed and mission-critical systems~\cite{fiala_2012_sdc,riesen_sandia_2010,lefray_2013_rsd,Cappello:09:Fault}. Based on this technique, each process is replicated across independent computing
nodes. When the original process fails, one of the replicas takes over the computation task. Replication can be used to detect and correct system failures that are otherwise undetectable,
such as silent data corruption \cite{ni_2013_acr} and Byzantine faults \cite{fiala_2012_sdc}. 
In addition, full and partial
replication have also been used to augment existing checkpointing techniques, and to guard
against silent data corruption \cite{stearly_2012_partial,elliott_2012_cpr}. There are several different implementations of
replication in the widely used MPI library, each with their different tradeoffs and overheads. The
overhead can be negligible or up to 70\% depending upon the communication patterns of the
application \cite{engelmann2011redundant}.

 %Similarly, the overhead of reexcution and replication-based techniques is a dominant factor of the computation and may lead to excessive delay and energy consumption in large-scale computing environments. 

The large increase in number of components significantly increases the propensity of extreme-scale computing systems to faults, while driving power consumption to unforeseen heights. Unfortunately, in so far as performance is concerned, resilience to failures and adherence to power budget constraints are two conflicting objectives, as achieving high performance may push the system's components past their thermal limit and increase their likelihood of failure.
In addition, abrupt and unpredictable changes in system behavior may lead to unexpected fluctuations in performance, which can be detrimental to applications' QoS requirements. The inherent vulnerability of extreme-scale computing systems, in terms of the envisioned high-rate and diversity of their faults, together with the demanding power constraints under which these systems will be designed to operate, calls for a radical reconsideration of the fault tolerance problem.

To this end, we propose an adaptive and power-aware resilience algorithm, referred to as \textit{Lazy
Shadowing}, as an efficient and scalable alternative to achieve high-levels of resilience, through
forward progress, in extreme-scale, failure-prone computing environments. In the proposed
approach, each process (referred to as main) is associated with a lazy replica (referred to as shadow) to improve resilience. The shadow executes the same code as its
associated main process, but at a reduced CPU rate to save power and energy. %The rate
%at which the shadow runs is derived based on the expected time-to-solution of the supported
%application, the power constraints imposed by the underlying computing infrastructure, and the
%likelihood of failure. 
Upon failure of the main process, the shadow increases its 
execution rate to complete the task, thereby reducing the impact of such a failure on the progress of
the remaining tasks. Successful completion of the main process, however, results in immediate
termination of the shadow. Since the failure rate of an individual component is much lower than that of 
the whole system, it is very likely that, most of the main processes complete their execution
successfully. %Consequently, the coupled execution of a main process and its associated shadow 
Consequently, the high probability that shadows never have to complete the full task, coupled with the fact that 
they initially only consume a minimal amount of energy, 
dramatically increases a power-constrained system's tolerance to failure, at a significantly reduced
energy consumption.

Conceptually, Lazy Shadowing can be viewed as a class of stochastically competitive algorithms, which 
achieve high level of resilience with a small relative performance penalty.
%guarantee small relative error with respect to the optimal expected cost, in terms of completion time and energy consumption. 
One gets different algorithms depending upon the main and shadow processes' execution rates, which are adaptive to the desired balance among three important objectives, namely, the expected completion time of the supported application, the power constraints imposed by the underlying computing infrastructure, and the tolerance of failure. %At the same time, we minimize the proposed algorithms' adaptivity to stochastic events by changing the shadow's execution rate only in response to a failure, in order to keep the algorithms simple as well as to reduce implementation overhead. 

The main contributions of this paper are as follows:
\begin{itemize}
	\item An adaptive, scalable, and power-aware algorithm, referred to as Lazy Shadowing, for fault tolerance in future extreme-scale, failure-prone computing systems.
	\item An evaluation framework composed of three analytical models to analyze the performance of Lazy Shadowing, compared to existing fault tolerance protocols.
	\item A thorough comparative study that shows the performance of Lazy Shadowing with different system characteristics and application requirements.
\end{itemize}


The rest of the paper is organized as follows. We begin with a survey on related work in Section 
\ref{sec:related_work}. Section \ref{sec:frame_model} introduces the Lazy Shadowing algorithm and Section \ref{frame_multiple} discusses how Lazy Shadowing can be used in 
extreme-scale computing environments for efficient fault tolerance. 
We then introduce three analytical models for performance assessment
in Section \ref{sec:analytical}, 
followed by experiments and evaluation in
section \ref{sec:evaluation}. Section \ref{sec:conclusion} concludes this work and points out 
future research directions.
