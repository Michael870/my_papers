As the scale and complexity of HPC continue to increase, both the failure rate and energy consumption are expected to increase dramatically, making it extremely challenging to deliver extreme-scale computing performance with satisfactory efficiency and reliability. Existing fault tolerance methods rely on either time or hardware redundancy. Neither of them appeals to the next generation of supercomputing, as the first approach may incur significant delay while the second one constantly wastes over 50\% of the system resources. The need for an efficient and reliable solution in extreme-scale, failure-prone computing environments calls for a reconsideration of the fault tolerance problem. 

To this end, we propose an adaptive and power-aware algorithm, referred to as Lazy Shadowing, as an efficient and scalable alternative to achieve high-levels of fault tolerance for future extreme-scale computing. In this paper, we present a comprehensive discussion of the techniques that enable Lazy Shadowing. In addition, we develop a series of mathematical models to assess its performance in terms of reliability, completion time, and energy consumption. 
Through comparison with existing fault tolerance approaches, we identify the scenarios where each of the alternatives should be chosen. Specially, checkpointing consumes the least time and energy when system size is small; process replication should be used to minimize completion time when system size is extremely large and failure rate is extremely high; and Lazy Shadowing is the choice for all other cases, for the consideration of both completion time and energy consumption. 
%In addition, we predict that Lazy Shadowing is able to achieve over 20\% of energy saving with potential completion time reduction for future extreme-scale computing. 

In the future, we will generalize our approach to multiple jobs. Initially we will assume that each job has an arrival time $a_j$, a workload $w_j$, and a deadline $d_j$ by which it must complete, and will seek minimally adaptive stochastically competitive strategies. The strategies in the case of no failure in~\cite{Albers:2011:MSS:1989493.1989539} is a good staring point. Another future direction is to apply our approach to servers with inter-processor power heterogeneity. In particular, it is likely that a chip will contain a few high-rate power-hungry processors, many low-rate power-efficient processors, and possibly an intermediate number of processors with intermediate rate and power efficiency. Generally speaking, inter-processor power heterogeneity is much harder to handle theoretically than intra-processor power heterogeneity. The main reason is that the number of jobs that can be run at high speed is fixed, and thus some jobs cannot be guaranteed to finish by their deadlines. A solution would be to allow a job to miss its deadline with some penalty, and to consider the objective as energy plus penalty. Again we seek to find stochastically competitive algorithms, and a good starting point may be the algorithms that are known to be competitive without faults~\cite{Kling:2013:PSM:2486159.2486183}.

