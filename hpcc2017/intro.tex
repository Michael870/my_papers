%The exponential increase of computation power, enabled by exascale computing infrastructure, is vital to accelerate discoveries across the spectrum of scientific fields and successfully address challenges in a broad spectrum of practical and increasingly complex applications that have profound impacts on society. To achieve the level of fidelity these applications require, the exascale computing infrastructure is expected to deliver 50 times the performance of today’s Petaflop HPC systems. Efforts are underway to meet this daunting challenge in the next decade. The path to exascale computing, however, involves several major road blocks and numerous challenges inherent to the complexity and scale of these systems. A key challenge for exascale computing systems stems from the stringent requirement, set by the US Department of Energy, to operate in a power envelope of 20 Megawatts. It is worth noting that simply scaling current HPC technology to achieve exascale performance will increase power consumption to 100 MW, resulting in unsustainable energy costs. Innovative design approaches, such as software and hardware co-design,  need to be explored to integrate all components of an exascale infrastructure and investigate trade-offs to increase computational power by a factor of 10, while limiting the increase of power consumption to only a factor of 1. 

%The trend toward shrinking transistor geometries, coupled with the variability in chip manufacturing, which is highly likely to decrease significantly the reliability of computing and communication components, presents yet another key challenge on the path toward exascale computing. The sheer number of components in future exascale computing, order of magnitudes higher than in existing HPC systems, will lead to frequent system failures, significantly limiting computational progress in large scale applications~\cite{exa_challenge_2010}. It is projected that the Mean Time Between Failures (MTBF) of future extreme-scale systems will be at the order of hours or even minutes, resulting in the occurrence of several failures on a daily basis~\cite{Bergman08exascalecomputing}.  New paradigms must be developed to cope with frequent faults and ensure that applications run to completion. The need to reduce time-to-solution for a wide range of compute- and data-intensive applications, while adhering to stringent power constraints and high-levels of resilience, further compound the exascale computing challenge. 

%The current approach to resilience relies on checkpointing and rollback recovery, whereby the execution state of an application is periodically saved to a stable storage, so that execution can be restarted from a recent checkpoint after a failure occurs~\cite{Elnozahy:02:Survey,kalaiselvi_sadhana_2000,Chandy:1985:DSD:214451.214456}. As the rate of failure increases, however, the time required to periodically checkpoint and restart an application approaches the system’s MTBF, which leads to a significant drop in the efficiency of checkpointing and rollback recovery~\cite{Cappello:2009:TER:1640402.1640428}. A number of approaches have been proposed to address this shortcoming, focusing on hybrid in-memory and multilevel checkpointing methods, which rely both on coordinated and uncoordinated checkpointing and the use of NVRAM and DRAM for storage, to reduce overhead and improve execution time~\cite{Gao:2015:RIC:2751205.2751212}. Although the additional power consumption remains low, the cost of additional memory may be prohibitive in exascale environments. Furthermore, the rollback to a consistent global state still requires that each node performs snapshots at different time and maintains a log of all incoming messages. 

The path to extreme scale computing involves several major road blocks and numerous challenges inherent to the complexity and scale of these systems. A key challenge stems from the enormous number of components, order of magnitudes higher than in existing HPC systems, which will lead to frequent system failures, significantly limiting computational progress. Another challenge stems from the stringent requirement, set by the US Department of Energy, to operate in a power envelope of 20 Megawatts~\cite{Bergman08exascalecomputing}. This puts into question the viability of traditional fault-tolerance methods and calls for a reconsideration of the resilience and power-awareness problems, at scale.

A common approach to resilience relies on time redundancy through checkpointing and rollback recovery. During normal execution, the computation state is periodically saved to a stable storage to allow recovery from a failure by restarting from a checkpoint. As the rate of failure increases, however, the time to periodically checkpoint and restart approaches the system's Mean Time Between Failures (MTBF), leading to a significant drop in efficiency as well as increase in power~\cite{4367962,1350776}. %A number of approaches have been proposed to address this shortcoming, focusing on hybrid in-memory and multilevel checkpointing methods~\cite{Gao:2015:RIC:2751205.2751212}. %, which rely both on coordinated and uncoordinated checkpointing. 
%Some of these methods require that each process maintain a log of all incoming messages. 

A second approach to resilience is replication, which exploits hardware redundancy by executing simultaneously multiple instances of the same task on separate processors~\cite{bartlett_1981_nonstop}. The physical isolation of processors ensures that faults occur independently, thereby enhancing tolerance to failure. This approach, however, suffers from low efficiency, as it dedicates 50\% of the computing infrastructure to the execution of replicas. Furthermore, achieving exascale performance, while operating within the 20 MW power cap, becomes challenging and may lead to high energy costs. 

%A second approach to achieve resilience in exascale, referred to as state machine replication, exploits hardware redundancy to overcome failures by executing simultaneously multiple instances of the same task on separate processors~\cite{bartlett_1981_nonstop}. The physical isolation of processors ensures that faults occur independently, thereby enhancing tolerance to failure. The approach, however, suffers low efficiency, as it dedicates 50\% of the computing infrastructure to the execution of replicas. Furthermore, achieving exascale performance, while operating within the 20 MW power cap, becomes challenging and may lead to high energy costs. 

To address these shortcomings, the {\it Shadow Replication} computational model, which explores radically different fault tolerance methodologies, has been proposed to achieve resilience in extreme-scale, failure-prone computing environments~\cite{mills2014power}. Its basic tenet is to associate each main process with a suite of coordinated shadow processes, %physically isolated from their associated main process, 
to deal with failures and meet the performance requirements of the underlying application. 
%The size of the suite and computational attributes of the shadows depend on the “elasticity” of the domain application with respect to computational fidelity, fault-tolerance and bounds on time-to-solution. In failure-prone exascale environments, full replication, whereby multiple shadows run in parallel as exact replicas of their associated main process may be necessary in order to comply with stringent requirements of the application.
For elastic applications, % whose performance requirements include multiple attributes, 
%it may be desirable to trade response time to reduce energy consumption. In this case, 
a single shadow that runs as a replica of its associated main, but at a lower execution rate, would be sufficient to achieve fault tolerance while maintaining acceptable response time. 
Based on this approach, the Lazy Shadowing scheme has been experimentally demonstrated to achieve higher performance and significant energy savings in comparison to existing approaches in most cases~\cite{cui_2016_scalcom}. 
%Similarly, trading fidelity to meet stringent response time requirements, in failure prone environment, can be achieved by running in parallel a single shadow, not as a full, but {\it differential} replica of its associated main. The shadow executes at the same speed as the main process, but at a lower computational resolution. 
%This approach is the basis of the Lazy Shadowing scheme~\cite{cui_2016_scalcom}, which strikes a balance between energy consumption and time-to-completion in error-prone exascale computing infrastructure. Experimental results demonstrate its ability to achieve higher performance and significant energy savings in comparison to existing approaches in most cases~\cite{cui_2016_scalcom}. 

Despite its ability to tolerate failure in a wide range of exascale computing environments, Lazy Shadowing assumes that either the main or the shadow fails, but not both. Consequently, the resilience of the system decreases as failure increases. Furthermore, when failure occurs, shadows are designed to substitute for their associated mains. The tight coupling and ensuing fate sharing between a main and its shadow increase the implementation complexity and reduce the efficiency of the system to deal with failures. 

In this paper, we introduce the new concept of {\it Rejuvenating Shadows} to reduce Lazy Shadowing's vulnerability to multiple failures and maintain consistent level of resilience. In this new model, shadows are no longer replicas, that are promoted to substitute for their associated mains upon a failure. Instead, each shadow is a ``rescuer" whose role is to restore the associated main to its exact state before failure. Rejuvenating Shadows can handle both transient and crash failures. The main contributions of this paper are as follows:


\begin{itemize}
{
	\item A new fault tolerance model, Rejuvenating Shadows, to deal with different types of failure and preserve resilience across failures.
   %\item Proposal of Rejuvenating Shadows as an enhanced scheme of Lazy Shadowing that incorporates rejuvenation techniques for consistent reliability.% and improved performance.
   \item A full-feature implementation of Rejuvenating Shadows into Message Passing Interface (MPI) to enhance its tolerance to failure at scale.
   \item The implementation of an application-level in-memory Checkpointing/Restart scheme, which is used for comparative study.
   \item A thorough evaluation and comparative analysis of the overhead and performance of the Rejuvenating Shadows, using multiple benchmark applications, under various failures.
}
\end{itemize}

The rest of the paper is organized as follows. A review of related work is provided in Section 
\ref{sec:related_work}. Section \ref{sec:reju_shadow} introduces fault model and system design, followed by discussion on implementation details in Section \ref{sec:implementation}.
Section \ref{sec:evaluation} presents empirical evaluation results. Section \ref{sec:conclusion} concludes this work and points out future directions.



