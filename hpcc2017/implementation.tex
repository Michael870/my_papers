%Implementation issues have been briefly discussed in \cite{cui_2016_scalcom}. 
This section presents the details of our implementation of rsMPI, which is an MPI library for Rejuvenating Shadows. 
Similar to rMPI and RedMPI~\cite{ferreira_sc_2011,fiala_2012_sdc}, rsMPI is implemented as a separate layer between MPI and user application. It uses the standard MPI profiling interface to intercept every MPI call (using function wrappers) and enforces Rejuvenating Shadows logic. In this way, we not only can take advantage of existing MPI performance optimization that numerous researches have spent years on, but also achieve portability across all MPI implementations that conform to the MPI specification.
When used, rsMPI transparently spawns the shadow processes during the initialization phase, manages the coordination between main and shadow processes, and guarantees order and consistency for messages and non-deterministic MPI events.
%Once completed, users should be able to link to the library without any change to existing codes. 

\subsection{MPI rank}
A rsMPI world has 3 types of identities: main process, shadow process, and coordinator process that coordinates between main and shadow. A static mapping between rsMPI rank and application-visible MPI rank is maintained so that each process can retrieve its identity. For example, if the user specifies $N$ processes to run, rsMPI will translate it into $2N + K$ processes, %where $K$ is the number of shadowed sets. Then the first $N$ ranks will be the mains, the next $N$ ranks be the shadows, and the last $K$ ranks be the coordinators. 
with the first $N$ ranks being the mains, the next $N$ ranks being the shadows, and the last $K$ ranks being the coordinators. 
We also allow the user to specify the collocation of shadows using a configuration file.  Figure~\ref{fig:mapping} shows the mapping between MPI ranks and rsMPI processes for the 3 shadowed sets corresponding to Figure~\ref{fig:logical_org}. Using the MPI profiling interface, we added wrapper for MPI\_Comm\_rank() and MPI\_Comm\_size(), so that each process (main or shadow) gets its correct execution path.

\begin{figure}[!t]
  \begin{center}
      \includegraphics[width=\columnwidth]{figures/mapping}
  \end{center}
  \caption{Mapping between MPI ranks and rsMPI processes for an example of 12 application-visible processes grouped into 3 shadowed sets. Dark blue squares are mains, light blue squares are shadows, and green rectangles are coordinators.}
  \label{fig:mapping}
\end{figure}

\subsection{Execution rate control}
\label{sec:rate_control}
While the mains always execute at maximum rate for HPC's throughput consideration, the shadows are configured to execute slower by collocation as specified by user in a configuration file. Accordingly, rsMPI will generate an MPI rankfile and provide it to the MPI runtime to control the process mapping. Note that rsMPI always maps the main and shadow of the same task onto different nodes. This is preferred as a fault on a node will not affect both of them. 
%In addition, rsMPI will automatically translate the number of processes (MPI ranks) specified for the application into the number of processes needed by rsMPI. For example, if the user specifies $N$ processes, rsMPI will translate it into $2N + K$ processes, where $K$ is the number of shadowed sets. Therefore, rsMPI will spawn $N$ main processes, $N$ shadow processes, and $K$ shadow coordinator processes for $K$ shadowed sets during MPI initialization. The logical organization is depicted in Figure~\ref{fig:logical_org}. 
To minimize resource usage, each coordinator is collocated with all the shadows in the shadowed set. Since a coordinator simply waits for incoming control messages (discussed below) and does minimal work, it has negligible impact on the execution rate of the collocated shadows. 


\subsection{Coordination between mains and shadows}
Each shadowed set has a coordinator process dedicated to coordination between the mains and shadows in the shadowed set. 
Coordinators do not execute any application code, but just wait for rsMPI defined control messages, and then carry out some 
coordination work accordingly. There are three types of control messages: termination, failure, and leaping. Correspondingly, each coordinator is responsible for three actions:
\begin{itemize}
  \item when a main finishes, it notifies its coordinator, which then forces the associated shadow to terminate.
  \item when a main fails, its associated shadow needs to catch up, so the coordinator temporarily suspends the other collocated shadows, and resumes their execution once the recovery is done.
  \item when a main initiates a leaping after detecting another main's failure, the coordinator triggers leaping at the associated shadow.
\end{itemize}
%Main and shadow processes can communicate with their coordinator via control messages defined by rsMPI. 
% Firstly, when a main process finishes, it notifies its coordinator, which then forces the associated shadow process to terminate. Secondly, when a main process fails, its associated shadow process needs to catch up, so the coordinator will temporarily suspend the other collocated shadows, and resume their execution once the recovery is done. Lastly, when a main or shadow initiates a leaping, either because of failure or buffer overflow, the coordinator triggers leaping at the associated shadow or main.  
%the RAS system will notify the corresponding shadow coordinator, which then promotes the associated shadow process to a new main process and kills the collocated shadows.
To separate control messages from data messages, rsMPI uses a dedicated MPI communicator for the control messages. This Control Communicator is created by the wrapper to the MPI\_Init call. In addition, to ensure fast response and minimize the number of messages, coordinators also use OS signals to communicate with their collocated shadows. %There are three types of coordination in rsMPI.

\subsection{Message passing and consistency}
We wrapped around every MPI communication function and implemented the consistency protocol described in Section~\ref{sec:shadow}. For sending functions, such as MPI\_Send() and MPI\_Isend(), rsMPI requires the main to duplicate the sending while the messages are suppressed at the shadow (see Figure~\ref{fig:cons_protocol}). For receiving functions, such as MPI\_Recv() and MPI\_Irecv(), both the main and the shadow does one receiving from the main process at the sending side. Internally, collective communication in rsMPI uses point-to-point communication in a binomial tree topology, which demonstrates excellent scalability.

We assume that only MPI operations can introduce non-determinism, and the SYNC message shown in Figure~\ref{fig:cons_protocol} is introduced to enforce determinism. For example, MPI\_ANY\_SOURCE may result in different message receiving orders between a main and its shadow. To deal with this, we serialize the receiving of MPI\_ANY\_SOURCE message by having the main first do the receiving and then use a SYNC message to forward the message source information to its shadow, which then issues a receiving from the specific source. Other operations, such as MPI\_Wtime() and MPI\_Probe(), are dealt with in a similar manner by forwarding the result from a main to its shadow.


\subsection{Leaping}
Checkpointing/restart requires each process to save its execution state, which can be used later to retrieve the state of the computation. Leaping is similar to saving the state in Checkpointing/restart, except that the state is transferred between a pair of main and shadow. 
To reduce the size of data involved in saving a process' state, we choose to implement leaping in the same way as application-level checkpointing~\cite{Beguelin97applicationlevel}. rsMPI provides a routine for users to register any data as process state. Application developer could use domain knowledge to identify only necessary state data, or use compiler techniques to automate this~\cite{5160999}. 
Specifically, rsMPI provides the following API for process state registration:

\begin{tabular}{ l l}
void & leap\_register\_state(void *addr, int count, \textbackslash \\
& MPI\_Datatype dt);
\end{tabular}

For each piece of data to be registered, three parameters are needed: a pointer to the address of the data, the number of data items, and the datatype. Internally, rsMPI uses a linked list to keep track of all registered data. %After each call of ``leap\_register\_state()", rsMPI will add a node to its internal linked list to record the three parameters. 
During leaping, the linked list is traversed to retrieve all registered data as the process state.

Coordination in leaping is simpler than in coordinated checkpointing, since leaping is always between a pair of main and shadow, while all processes need to coordinate for checkpointing. To synchronize the leaping between a main and a shadow, the coordinator in the corresponding shadowed set is involved. For example, when a main detects failure of another main and initiates a leaping, it will send a control message to its coordinator, which then uses a signal to notify the associated shadow to participate in the leaping. 

Different from Checkpointing where the process state is saved, leaping directly transfers process state between a main and its shadow. 
Since MPI provides natural support for message passing between processes, 
rsMPI uses MPI messages to transfer process state. Although multiple pieces of data can be registered as a process' state, only a single message is needed to transfer the process state, as MPI supports derived datatypes. To prevent the messages carrying process state from mixing with application messages, rsMPI uses a separate Control Communicator for transferring process state. With the synchronization of leaping by coordinator and the fast transfer of process state via MPI messages, the overhead of leaping is minimized. 

A challenge in leaping lies in the need for maintaining state consistency. To make sure a pair of main and shadow stay consistent after a leaping, not only user-defined states should be transferred correctly, but also lower level states, such as program counter and message buffer, need to be updated correspondingly. Specifically, the roll forward process needs to satisfy two requirements. 
Firstly, after leaping the roll forward process should discard all obsolete messages before resuming normal execution. Secondly, the roll forward process should resume execution at the same point as the target process. We discuss our solutions below, under the assumption that the application's main body consists of a loop, which is true in most cases. 
%Firstly, after updating its state, the lagging process should resume execution at the same point as the target process. Secondly, the lagging process should discard all obsolete message before resuming normal execution. To address these issues, first we assume that the application's main body consists of a loop, which is true in most cases.

There is no straightforward way to discard obsolete messages since the message buffer is maintained by MPI runtime and not visible to rsMPI. Hence, rsMPI borrows the idea of "determinants" used in rollback recovery to correctly discard all obsolete messages. Specifically, during normal execution, both the main and shadow record the meta data (i.e., MPI source, tag, and communicator) for all received messages in the receiving order. During leaping, the meta data at the main is transferred to the shadow, so that the shadow knows about the messages that have been received by its main but not by itself. Then the shadow combines MPI probe and MPI receive operations to remove the messages from MPI runtime buffers in the correct order.

To satisfy the second requirement (resume execution from the same state), we restrict leaping to always occur at certain possible points, and use internal counter to make sure that both the roll forward and target processes start leaping from the same point. For example, when a main initiates a leaping, the coordinator will trigger a specific signal handler at the associated shadow. The signal handler does not carry out leaping, but sets a flag for leaping and receives from its main a counter value that indicates the leaping point. Then, the shadow will check the flag and compare the counter value at every possible leaping point. Only when both the flag is set and counter value matches will the shadow start leaping. In this way, it is guaranteed that after leaping the main and shadow will resume execution from the same point. To balance the trade-off between implementation overhead and  flexibility, we choose MPI receive operations as the only possible leaping points. 

 

%Alternatively, we choose to remove obsolete message from message buffer by having the process execute all the skipped MPI communication routines after it finishes leaping. To achieve this, we require the user to define a function for the MPI communication functions used in the application's main body loop. The function should have two parameters to specify the starting and ending index for skipped iterations. In addition, the user needs to register the function with rsMPI with the following library call:

%void leap\_register\_func(void (*func)(int, int));

%To discard all obsolete messages after leaping, the process that updates its process state will call the registered function, for which the two parameters will be automatically specified by rsMPI. Essentially, it executes all the  MPI communication functions from the skipped iterations and consumes all the useless messages.  

%\subsection{Failure injection and detection}
%As one main goal of this work is to achieve fault tolerance, an integrated fault injector is required to evaluate the effectiveness and efficiency of rsMPI to tolerate failures during execution. To produce failures in a manner similar to naturally occurring process failures, our failure injector is designed to be distributed and co-exist with all rsMPI processes. Failure is injected by sending a specific signal to the target process.

%Failure detection is beyond the scope of rsMPI, and we assume the underlying hardware platform has a RAS system that provides this functionality. In our prototype, we emulate a RAS system with a signal handler installed at every main and shadow process. The signal handler catches failure signal from failure injector, and uses a rsMPI defined failure message via a dedicated communicator to notify all other processes of its failure. Similar to ULFM, processes in rsMPI can detect failure only when it does an MPI receive operation. When starting a rsMPI receive, rsMPI checks for failure messages before it does the actual MPI receive operation.

%\subsection{Double in-memory checkpointing}
%We also implemented checkpointing to compare with rsMPI in the presence of failures. To be optimistic, we chose double in-memory checkpointing that is much more scalable then disk-based checkpointing~\cite{zheng2004ftc}. Same as leaping in rsMPI, our implementation provides an API for process state registration. This API requires the same parameters as leap\_register\_state(void *addr, int count, MPI\_Datatype dt), but internally, it needs to allocate extra memory in order to store the state of a ``buddy" process. Another provided API is checkpoint(), which can be used to insert a checkpoint in the application code. For fairness, our implementation also uses MPI messages to transfer state between buddies.  
