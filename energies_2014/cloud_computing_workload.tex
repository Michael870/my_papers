%\noindent 

Today's popular online services, such as web search, video streaming,
and social networks, are all powered by large data centers. In
addition to these public services, high-performance applications and
business activities are migrating onto cloud computing~\cite{Ferdman:2012:CCS:2150976.2150982,mrbs}. Table \ref{tbl:apps}
lists several classes of cloud computing applications. 

\begin{table}[!h]
	\caption{Typical cloud computing applications.}
	\centering
		\begin{tabularx}{\columnwidth}{|l|X|}
			\hline
			Class                          & Examples                         \\
			\hline
			Data Analytics   & Bioinformatics\\ 
			& Business Intelligence\\
			\hline
			Graph Analytics  & Social Networks\\ 
			& Recommendation Systems\\
			\hline
			Web Search       & Text Processing\\
			& Recommendation Systems\\
			\hline
			\end{tabularx}
	\label{tbl:apps}
\end{table}

Two main characteristics of the above applications are large dataload
and high parallelism. In 2008, Google processed 20 PB of data with
MapReduce each day; in April 2009, a blog revealed eBay's 2 enormous
data warehouses: one with 2 PB of data and the other with 6.5 PB of
data; shortly thereafter, Facebook announced that a 2.5
Peta-bytes of user data, at a growth rate of at 15 Tera-bytes per day~\cite{lin2010data}. 
Without massive parallelism, handling such volumes of data is beyond the processing capability of curent computing infrastructre. 

Each job, that involves processes these data, typically requires multiple
phases that execute in sequence. During each phase, workload is
splited into tasks and processed in parallel to speed up the whole
process.  Consequently, a cloud computing job can be modeled as 
a set of tasks, executing in parallel on different computing nodes. For the job to complete, all tasks must finish their exectution. 
Consequently, if a task is delayed then all remaining tasks must wait, until the delayed task completes. 
This model is directly reflective of the \emph{MapReduce}
computational model, which is predominately used in Cloud
Computing \cite{mrbs}.  

Each job has a targeted response time defined
by the terms of the SLA. Further, the SLA defines the amount to be
paid for completing the job by the targeted response time as well as
the penalty to be incurred if the targeted response time is not
met. 

Each task is mapped to one compute core and executes at a speed, $\sigma$. The partition of the job among tasks is
such that each task processes a similar
workload, $W$. Consequently, baring failures, tasks are expected to
complete at about the same time. Therefore, the minimal response time
of each task, when no failure occurs, is
$t_{min}~=~\frac{W}{\sigma_{max}}$, where $\sigma_{max}$ is the maximum speed. This is also the minimal response
time of the entire phase. 

As the number of tasks increases, however, the likelihood of a task
failure during an execution of a given phase increases
accordingly. This underscores the importance of an energy-efficient
fault-tolerance model to mitigate the impact of a failing task on the
overall delay of the execution phase. The following section describes
Shadow Replication, a fault-tolerant, energy-aware computational model to achieve profit-maximizing resiliency in cloud computing.
