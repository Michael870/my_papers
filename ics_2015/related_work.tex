%Extreme-scale computing presents some unique challenges to fault tolerance as faults are no longer 
%an exceptional event \cite{ferreira_sc_2011}. 
Rollback and recovery are predominate mechanisms to achieve fault
tolerance in current HPC environments. In the most general form, the rollback and recovery mechanism
involves the periodic saving of the current system state to stable storage, with the anticipation that
in the case of a system failure, computation can be restarted from the most recently saved state \cite{Elnozahy:02:Survey}. %The identification of an error, before or during a checkpoint,
%requires that the application rollback to the previously completed checkpoint. 
Coordinated
Checkpoint is a method to achieve a globally consistent state. Specifically, all processes
coordinate with one another to produce individual states that satisfy the ``happens before"
communication relationship \cite{chandy_trans_1972}, which is proved to provide a consistent global state.
%Essentially, the algorithm provides a method for all processes involved to stop operation ``at the same
%time" and transfer their system state to a stable storage. 
The major benefit of coordinated
checkpointing stems from its simplicity and ease of implementation. Its major drawback, however, is the
lack of scalability, as it requires global process coordination to achieve a checkpoint 
\cite{elnozahy_dsc_2004,riesen_sandia_2010,
hargrove2006berkeley}.


In uncoordinated checkpointing, processes record their states independently and postpone creating a 
globally consistent view until the recovery phase \cite{plank_ftc_1999}. The major advantage is the reduced overhead during fault free operation. However, the scheme requires that
each process maintains multiple checkpoints and message logs, necessary to construct a consistent 
state during recovery. It can also suffer the well-known domino effect, which may result in the 
re-execution of the entire application \cite{randell_domino_effect}. One hybrid approach, known as communication induced 
checkpointing schemes, aims at reducing coordination overhead \cite{alvisi_ftc_1999}. The approach, however, may 
cause processes to store useless states that are never used in future rollbacks. To address this 
shortcoming, ``forced checkpoints" have been proposed \cite{helary_rds_1997}. This approach may lead to unpredictable
checkpointing rates. Although well-explored, uncoordinated checkpointing has not been widely adopted
in HPC environments, due to its dependency on applications \cite{zheng_2004_ftccharm,guermouche_2011_ipdps}.


One of the largest overheads in any checkpointing process is the time necessary to write the checkpointing 
to stable storage, during which the application pause its execution. Incremental checkpointing attempts
to address this shortcoming by only writing the changes since the previous checkpoint \cite{nam_ftc_1997,Agarwal:04:Adaptive}. This
can be achieved using dirty-bit page flags \cite{plank_ftcs_1994,elnozahy_1992_manetho}. Hash based incremental checkpointing, on the
hand, makes use of hashes to detect changes \cite{nam_ftc_1997,Agarwal:04:Adaptive}. Another proposed scheme, known as copy-on-write,
offloads the checkpointing process to a secondary task and only writes incremental checkpoints \cite{li_trans_1994}.
The main concern of these techniques is that some applications would require increase in their
memory to support the simultaneous execution of the checkpointing and the application. It has been suggested that nodes in extreme-scale systems should be configured with fast local storage, which
improves the performance of checkpointing \cite{doe_ascr_exascale_2011}. Multi-level checkpointing, which consists of
writing checkpoints to multiple storage targets, can benefit from such a strategy \cite{Moody:10:SCR,hakkarine_2013}. This,
however, may lead to increased failure rates of individual nodes and increased per-node cost \cite{chen_2011_hystor}.
Furthermore, it may complicate the checkpoint writing process and requires that the system track the
current location of all process's checkpoints.


Process and state machine replication have long been used to provide fault tolerance in distributed and mission critical systems. % \cite{schneider_1990_tutorial,bartlett_1981_nonstop}. 
To ensure consistent system state, all process messages must
be delivered to all nodes running a given process, typically done by using a message ordering protocol 
\cite{lamport_trans_1998}. Replication can be used to detect and correct system failures that are otherwise undetectable,
such as silent data failures \cite{ni_2013_acr} and Byzantine faults \cite{fiala_2012_sdc}. %Replication was proposed as a
%viable alternative to checkpointing in HPC \cite{riesen_sandia_2010,lefray_2013_rsd,Cappello:09:Fault}. 
Full and partial
replication have also been proposed, to augment existing checkpointing techniques, and to guard
against silent data corruption \cite{stearly_2012_partial,elliott_2012_cpr}. %There are several different implementations of
%replication in the widely used MPI library, each with their different tradeoffs and overheads. The
%overhead can be negligible or up to 70\% depending upon the communication patterns of the
%application [25]. Moreover, replication alone is not enough to guarantee fault tolerance since
%it is possible that all nodes executing a given process could fail simultaneously, thus
%replication is typically paired with some form of checkpointing. 
To the best of our knowledge,
Lazy Shadowing is the first attempt to explore a state-machine replication based framework
that provides a tradeoff between time and hardware redundancy while meeting resilience and
power requirements.