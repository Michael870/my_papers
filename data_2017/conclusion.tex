Scientific research and engineering development are increasingly relying on computational modeling, simulation, and data analytics to augment theoretical analysis. Behind the wheel, data has become the ultimate driving force that yields insights and propels innovation. With a torrent of data generated every second from distributed sensors, social media, software logs and so on, it is critical to analyze the data in a timely manner,  at massively parallel scale, and with fault tolerance capabilities. 

Shadow Computing is a novel fault-tolerant computational model that unifies HPC and Big Data analytics. The flexibility within the model allows it to embrace different optimization techniques in accordance with the underlying workloads, whether compute-intensive or data-intensive. Furthermore, different execution strategies can be applied to deal with different types of failures, or multiple types of failures at the same time. 

By exploring the interplay between performance, fault-tolerance, and energy consumption, Shadow Computing is predicted to save a significant amount of energy (up to 64.9\%) compared to existing fault tolerance approaches, while respecting strict response time requirements and maintaining the save level of resilience. In the future, we plan to implement this model and perform intensive empirical evaluation to verify the accuracy of the analytical models.     