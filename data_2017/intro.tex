Modern scientific discoveries rely heavily on large-scale
simulation and data analytics. To enable future scientific breakthroughs, the next generation of scientific applications will require
Exascale computing performance ($10^{18}$ FLOPS) to support the execution of predictive models and analysis of massive quantities of
data, with significantly higher resolution and fidelity than what is possible within existing computing infrastructures. 
In order to deliver the desired performance of these applications, Exascale computing infrastructure will require one million processors, each supporting 1,000 cores, resulting in a billion-core computing infrastructure while also requiring a dramatic increase in the number of memory modules, communications devices and storage components~\cite{Bergman08exascalecomputing}.

The dramatic increase in the number of components brings about several daunting scalability challenges. With the explosive growth in the system scale, there will be a significant increase in the propensity to faults, and the power consumption will rise to unforeseen heights. Unfortunately, in so far as performance is concerned,
resilience to faults and adherence to power budget constraints are two conflicting objectives, as achieving high
performance may push the system’s components past their thermal limit and increase their likelihood of failure. 

System power has become a leading design constraint on the path to exascale computing. To mitigate the operational expenditure, U.S. DoE has set 20 MW  as the power cap for a full exascale-capable system, challenging the HPC community to design future systems capable of sustaining 50 GFlops
per Watt (GF/W). Currently, the most energy-efficient system on the Green500 list is the new TSUBAME 3.0 at the Tokyo Institute of Technology. It achieved 14.110 GF/W during its 1.998-petaflop Linpack performance run. To enable future exascale system, combined efforts from hardware, OS, and software must improve energy efficiency by a factor of over 3.5X.

Due to the complexity and scale of future exascale systems, another major road-block is the increasing propensity of the system to the diverse types of failures, including crash failure and silent data corruption. Regardless of the individual components’ reliability, system resilience will continue to decrease as the number of components increases. By avoiding the full re-execution of a failing application, checkpoint-based techniques have been the dominant fault tolerance mechanism in HPC in the past 30 years. Recent work, however, show that existing solutions are likely not to scale to the level of faults anticipated in exascale environments. Furthermore, the nature and diversity of errors in exascale computing are such that checkpoint-restart may not be an adequate approach. Based on this observation, several studies have proposed state machine replication as a scalable solution to handling diverse types of faults~\cite{fiala2012detection,ferreira2011evaluating}. 

Scientific discoveries typically involve 4 stages--theory, experiment, computation, and data, in a closed loop. Theory suggests hypotheses that are verified through Experiment, and theory is developed and explored through computation. Both experiment and computation generate data, and data analytics discovers hypotheses and drive theory.
The sheer volume of data and its growing heterogeneity increasingly drive the need for more capable data-intensive computing platforms~\cite{6842585}. As a result, 
big data analytics in HPC become an emerging research area. \zh{Can make a stronger case here with an example application} 
%The sheer volume of data and its growing heterogeneity increasingly drive the need for more capable data-intensive computing platforms. As a result, the boundaries between HPC and Big Data analytics continues to blur.

Despite the growing importance of big data analytics in the HPC community, the hardware, architecture, and software are still designed for compute-intensive applications, in which computations have spatial and temporal locality and problems usually fit into memory. Challenges remain in achieving maximum performance for data-centric applications on platforms that are engineered for compute-intensive jobs. 

Recognizing this problem, we propose a systematic computational model, referred to Shadow Computing, that achieves power-aware fault tolerance for both compute- and data-intensive applications. Similar to state machine replication, Shadow Computing replicates each process to achieve redundancy. However, in Shadow Computing replicas are assigned different roles (main process and shadow process) with different resource allocation and dynamic rate adjustment in order to maximize resource utilization and performance.  
Shadow Computing is a flexible model that covers a spectrum of fault tolerance strategies. including re-execution and state machine replication. By taking into consideration the nature of the underlying application and the types of failure, Shadow Computing is able to achieve a desired trade-off among multiple competing objectives, such as performance, resilience, and power budget.  

The main challenge in Shadow Computing reside in jointly determining the resource allocation and execution rates for different roles of the replicas, with the objective to minimize time and power, while satisfying resource constraints. 
In previous works, we studied the application of the Shadow Computing model to compute-intensive workloads in HPC, in which communication and synchronization may be frequent~\cite{6787325,en7085151,7816907}. Both analytical models and empirical experiments demonstrate that it can achieve higher performance and significant energy savings in comparison to existing approaches in most cases. This paper focuses on data-intensive applications and presents novels schemes within the Shadow Computing model to handle multiple types of failures. The main contributions of this paper are as follows:
\begin{itemize}
	\item A scheme designed for tolerating crash failures, and correspondingly an optimization framework that minimizes energy consumption while maintaining acceptable response time.
    \item A scheme designed for tolerating silent data corruption, and correspondingly an optimization framework that minimizes energy consumption while maintaining acceptable response time.
    \item An evaluation framework that demonstrates Shadow Computing's power awareness and energy savings compared to state-of-the-art approach.
    \item A discussion on simultaneously tolerating multiple types of failures. 
\end{itemize}

This paper begins by describing an execution model typically used for processing data-parallel tasks (Section~\ref{sec:execution_framework}). Then we discuss two novel schemes that apply the Shadow Computing model to deal with two different types of failures in Section~\ref{sec:crash_collocation} and \ref{sec:silent_cp_leap}, and present experiment results in Section~\ref{sec:eval}. Finally, Section~\ref{sec:multiple} explores the tolerance of multiple failures, Section~\ref{sec:related_work} surveys related work, and Section~\ref{sec:conclusion} concludes.