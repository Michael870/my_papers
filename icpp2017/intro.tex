%The exponential increase of computation power, enabled by exascale computing infrastructure, is vital to accelerate discoveries across the spectrum of scientific fields and successfully address challenges in a broad spectrum of practical and increasingly complex applications that have profound impacts on society. To achieve the level of fidelity these applications require, the exascale computing infrastructure is expected to deliver 50 times the performance of today’s Petaflop HPC systems. Efforts are underway to meet this daunting challenge in the next decade. The path to exascale computing, however, involves several major road blocks and numerous challenges inherent to the complexity and scale of these systems. A key challenge for exascale computing systems stems from the stringent requirement, set by the US Department of Energy, to operate in a power envelope of 20 Megawatts. It is worth noting that simply scaling current HPC technology to achieve exascale performance will increase power consumption to 100 MW, resulting in unsustainable energy costs. Innovative design approaches, such as software and hardware co-design,  need to be explored to integrate all components of an exascale infrastructure and investigate trade-offs to increase computational power by a factor of 10, while limiting the increase of power consumption to only a factor of 1. 

%The trend toward shrinking transistor geometries, coupled with the variability in chip manufacturing, which is highly likely to decrease significantly the reliability of computing and communication components, presents yet another key challenge on the path toward exascale computing. The sheer number of components in future exascale computing, order of magnitudes higher than in existing HPC systems, will lead to frequent system failures, significantly limiting computational progress in large scale applications~\cite{exa_challenge_2010}. It is projected that the Mean Time Between Failures (MTBF) of future extreme-scale systems will be at the order of hours or even minutes, resulting in the occurrence of several failures on a daily basis~\cite{Bergman08exascalecomputing}.  New paradigms must be developed to cope with frequent faults and ensure that applications run to completion. The need to reduce time-to-solution for a wide range of compute- and data-intensive applications, while adhering to stringent power constraints and high-levels of resilience, further compound the exascale computing challenge. 

%The current approach to resilience relies on checkpointing and rollback recovery, whereby the execution state of an application is periodically saved to a stable storage, so that execution can be restarted from a recent checkpoint after a failure occurs~\cite{Elnozahy:02:Survey,kalaiselvi_sadhana_2000,Chandy:1985:DSD:214451.214456}. As the rate of failure increases, however, the time required to periodically checkpoint and restart an application approaches the system’s MTBF, which leads to a significant drop in the efficiency of checkpointing and rollback recovery~\cite{Cappello:2009:TER:1640402.1640428}. A number of approaches have been proposed to address this shortcoming, focusing on hybrid in-memory and multilevel checkpointing methods, which rely both on coordinated and uncoordinated checkpointing and the use of NVRAM and DRAM for storage, to reduce overhead and improve execution time~\cite{Gao:2015:RIC:2751205.2751212}. Although the additional power consumption remains low, the cost of additional memory may be prohibitive in exascale environments. Furthermore, the rollback to a consistent global state still requires that each node performs snapshots at different time and maintains a log of all incoming messages. 

The path to extreme scale computing involves several major road blocks and numerous challenges inherent to the complexity and scale of these systems. A key challenge stems from the stringent requirement, set by the US Department of Energy, to operate in a power envelope of 20 Megawatts. Another challenge stems from the huge number of components, order of magnitudes higher than in existing HPC systems, which will lead to frequent system failures, significantly limiting computational progress~\cite{Bergman08exascalecomputing}. This puts into question the viability of traditional fault-tolerance methods and calls for a reconsideration of the fault-tolerance and power-awareness problem, at scale.

A common approach to resilience relies on time redundancy through checkpointing and rollback recovery. Specifically, the execution state is periodically saved to a stable storage to allow recovery from a failure by restarting from a checkpoint either on a spare or on the failed processor after rebooting it. As the rate of failure increases, however, the time to periodically checkpoint and rollback approaches the system's Mean Time Between Failures (MTBF), which leads to a significant drop in efficiency and increase in power~\cite{Elnozahy:02:Survey,kalaiselvi_sadhana_2000,Chandy:1985:DSD:214451.214456}. A number of approaches have been proposed to address this shortcoming, focusing on hybrid in-memory and multilevel checkpointing methods~\cite{Gao:2015:RIC:2751205.2751212}, which rely both on coordinated and uncoordinated checkpointing. Some of these methods require that each process maintain a log of all incoming messages. 

A second approach to resilience is replication, which exploits hardware redundancy by executing simultaneously multiple instances of the same task on separate processors~\cite{bartlett_1981_nonstop}. The physical isolation of processors ensures that faults occur independently, thereby enhancing tolerance to failure. This approach, however, suffers from low efficiency, as it dedicates 50\% of the computing infrastructure to the execution of replicas. Furthermore, achieving exascale performance, while operating within the 20 MW power cap, becomes challenging and may lead to high energy costs. 

%A second approach to achieve resilience in exascale, referred to as state machine replication, exploits hardware redundancy to overcome failures by executing simultaneously multiple instances of the same task on separate processors~\cite{bartlett_1981_nonstop}. The physical isolation of processors ensures that faults occur independently, thereby enhancing tolerance to failure. The approach, however, suffers low efficiency, as it dedicates 50\% of the computing infrastructure to the execution of replicas. Furthermore, achieving exascale performance, while operating within the 20 MW power cap, becomes challenging and may lead to high energy costs. 

To address these shortcomings, the {\it Shadow Replication} computational model, which explores radically different fault tolerance methodologies, has been proposed to address resilience in extreme-scale, failure-prone computing environments~\cite{mills2014power}. Its basic tenet is to associate with each main process a suite of coordinated shadow processes, physically isolated from their associated main process to hide failures, ensure system level consistency and meet the performance requirements of the underlying application. 
%The size of the suite and computational attributes of the shadows depend on the “elasticity” of the domain application with respect to computational fidelity, fault-tolerance and bounds on time-to-solution. In failure-prone exascale environments, full replication, whereby multiple shadows run in parallel as exact replicas of their associated main process may be necessary in order to comply with stringent requirements of the application.
For elastic applications whose performance requirements include multiple attributes, 
%it may be desirable to trade response time to reduce energy consumption. In this case, 
a single shadow that runs as a replica of its associated main, but at a lower execution rate, would be sufficient to achieve acceptable response time. 
%Similarly, trading fidelity to meet stringent response time requirements, in failure prone environment, can be achieved by running in parallel a single shadow, not as a full, but {\it differential} replica of its associated main. The shadow executes at the same speed as the main process, but at a lower computational resolution. 
This Lazy Shadowing scheme, which is described in~\cite{cui_2016_scalcom}, strikes a balance between energy consumption and time-to-completion in error-prone exascale computing infrastructure. Experimental results demonstrate its ability to achieve higher performance and significant energy savings in comparison to existing approaches in most cases. 

Despite its viability to tolerate failure in a wide range of exascale systems, Lazy Shadowing assumes that either the main or the shadow fails, but not both. Consequently, the resiliency of the system decreases as failure increases. Furthermore, when failure occurs, shadows are designed to substitute for their associated mains. The tight coupling and ensuing fate sharing between a main and its shadow increase the implementation complexity of Lazy Shadowing and reduce the efficiency of the system to deal with failures. 

In this paper, we introduce the new concept of {\it Rejuvenating Shadows} to reduce Lazy Shadowing's vulnerability to multiple failures and enhance its performance. In this new model, shadows are no longer replicas, which are promoted to substitutes for their associated main processes, upon the occurrence of a failure. Instead, each shadow is a {\it rescuer} whose role is to restore the associated main to its exact state before failure. 

Rejuvenation ensures that the vulnerability of the system to failure does not increase after a failure occurs. Furthermore, it can handle different types of failure, including transient and crash failures. The main contributions of this paper are as follows:


\begin{itemize}
{
   \item Proposal of Rejuvenating Shadows as an enhanced scheme of Lazy Shadowing that incorporates rejuvenation techniques for consistent reliability and improved performance.
   \item A full-feature implementation of Rejuvenating Shadows for Message Passing Interface.
   \item An implementation of application-level in-memory Checkpointing/Restart for comparison.
   \item A thorough evaluation of the overhead and performance of the implementation with multiple benchmark applications and under various failures.
}
\end{itemize}

The rest of the paper is organized as follows. We begin with a survey on related work in Section 
\ref{sec:related_work}. Section \ref{sec:reju_shadow} introduces fault model and system design, followed by discussion on implementation details in Section \ref{sec:implementation}.
Section \ref{sec:evaluation} presents empirical evaluation results. Section \ref{sec:conclusion} concludes this work and points out future directions.



