In a complex system like the supercomputers we have today, the performance of Leaping Shadows is subject to both the hardware configuration, such as failure detection ability and the execution rate control mechanism, and software behavior, such as communication patterns and the amount of synchronization. 
It's extremely difficult for an analytical framework to precisely capture every details of Leaping Shadows when it runs in a real environment. Therefore, a functional prototype is necessary to prove its validity as well as measure its actual performance. 

As a proof of concept, we implement Leaping Shadows in a runtime library, referred to as rsMPI, for Message Passing Interface (MPI), which is the de facto programming paradigm for HPC. With the understanding that MPI standard keeps evolving to support new features and semantics, we focus on the most essential aspects and make srMPI MPI-1 compliant. Currently, rsMPI focuses on tolerating crash failures and associates one shadow process with each main process. %Collocation is used to achieve the desired execution rate of each shadow. 
With the rejuvenation technique introduced in Chapter~\ref{chapter:scale}, applying Leaping Shadows with double modular redundancy minimizes the hardware and power costs, while being able to tolerate multiple failures. It is to be noted that, it is straightforward to extend this implementation to associate two shadows with each each main, for the purpose of dealing with silent failures, as discussed in Chapter~\ref{chapter:sdc}. 


Instead of a full-feature MPI implementation, rsMPI library is designed to be a separate layer between MPI and user application, and uses function wrappers on top of the MPI profiling hooks to intercept every MPI call. This is similar to the rMPI and redMPI implementations~\cite{ferreira_sc_2011,fiala_2012_sdc}. There are three benefits for this choice: 1) we can save tremendous time and efforts of rebuilding MPI from scratch; 2) we can take advantage of existing MPI performance optimization that numerous researches have spent years on; and 3) the library is portable across all MPI implementations. 
When linked to an MPI application, rsMPI transparently spawns the shadow processes at the initialization phase, manages the coordination between main and shadow processes during execution, and guarantees order and consistency for messages and non-deterministic events.
%Once completed, users should be able to link to the library without any change to existing codes. 

%Currently, rsMPI focuses on tolerating crash failures and associates one shadow process with each main process. With the rejuvenation technique introduced in Chapter~\ref{chapter:scale}, applying Leaping Shadows with double modular redundancy minimizes the hardware and power costs while being able to tolerate multiple failures. It is to be noted that, it is straightforward to extend this implementation to associate two shadows with each each main, for the purpose of dealing with silent failures, as discussed in Chapter~\ref{chapter:sdc}. To achieve the desired execution rates for shadow processes, rsMPI uses shadow collocation, since this further reduces the hardware and power costs.

To avoid the drawbacks of DVFS, current rsMPI applies collocation to achieve the desired execution rates for the shadows. 
While each main executes on a separate processor at maximum rate for HPC's throughput consideration, shadows are configured to collocate and execute at a slower rate based on a user configuration file. Accordingly, rsMPI will generate an MPI rankfile and provide it to the MPI runtime to control the process to processor mapping. Note that rsMPI always maps the main and shadow of the same task onto different nodes. This is required to prevent a fault on one node from affecting  both a main and its associated  shadow.


Despite the overview of the rsMPI library above, many challenges lie in efficiently integrating MPI with Leaping Shadows to support fault tolerance with high performance. One challenge is the preservation of the rich communication semantics in MPI standard. To be MPI-1 compliant, rsMPI needs to support both point-to-point and collective communications, in both blocking and non-blocking manners. A second challenge lies in coordination between the main and shadow process, in order to correctly terminate, carry out leaping, and perform failure recovery. Another challenge is that state consistency between mains and shadows needs to be maintained all the time. Last but not least, the side effects of divergence need to be addresses. %Specifically, there shall be no message buffer overflow; and the slower shadows shall not impact the performance of mains.
The following sections discuss solutions to the above challenges by presenting the rsMPI implementation details. 

\section{Function wrappers}

As illustrated in Figure~\ref{fig:layer_arch}, rsMPI is positioned between the MPI runtime layer and the user application layer. Essentially, rsMPI a library of function wrappers, one for each MPI primitive. As a result, user applications can be linked to rsMPI without modification. When the user application makes a MPI function call, rsMPI library intercepts this call to enforce the Leaping Shadows logic. Internally, rsMPI uses the original MPI procedures by interacting with the MPI profiling interface. For example, to hide the existence of shadows from the user application, rsMPI adds a wrapper for the MPI\_Comm\_size() function, which is to retrieve the number of processes (i.e., MPI ranks) in an MPI communicator. When the user application calls MPI\_Comm\_size(), rsMPI internally calls PMPI\_Comm\_size() using the MPI profiling interface to retrieve \textit{size}, counting both mains and shadows. Then rsMPI returns \text{size/2} to the above application, giving an illusion that the same number of processes are launched. This is illustrate in Figure~\ref{fig:layer_arch_22}.


\begin{figure}[!h]
  \begin{center}
  	\subfigure[Position of the rsMPI library in software stack.]
		{
			\label{fig:layer_arch_11}
      		\includegraphics[width=0.45\columnwidth]{Figures/layer_arch_11}
      	}
  	\subfigure[Example of MPI\_Comm\_size() function call.]
		{
			\label{fig:layer_arch_22}
      		\includegraphics[width=0.45\columnwidth]{Figures/layer_arch_22}
      	}
  \end{center}
  \caption{rsMPI is inserted between the MPI and application layers to intercept MPI calls from the above application.}
  \label{fig:layer_arch}
\end{figure}

A static mapping between rsMPI ranks and application-visible MPI ranks is maintained so that each process can retrieve its identity. For example, if the user specifies $N$ ranks to launch, rsMPI will automatically translate it into $N + N$ ranks, with the first $N$ ranks being the mains, and the next $N$ ranks being their associated shadows in the corresponding order. By maintaining this main to shadow mapping, rsMPI guarantees that each process (main or shadow) gets its correct execution path. 
This logic is enforced by wrapping the MPI\_Comm\_rank() function. %so that each process (main or shadow) gets its correct execution path.
The wrappers for the MPI communication primitives will be discussed in the next section, when we introduce the consistency protocols.

\section{Message passing and consistency}
\label{sec:msg_consistency}
State consistency between mains and shadows is required both during normal execution and following a failure of a main process to roll-forward the shadows. 
As depicted in Figure~\ref{fig:cons_protocol}, two consistency protocols are explored  
to enforce sequential consistency, i.e., each shadow sees the same message order and operation results as its main. In the sender-forwarding protocol, as shown in Figure~\ref{fig:cons_protocol_1}, each main sender is responsible for forwarding each application message to the shadow of the receiver. In the receiver-forwarding protocol, as shown in Figure~\ref{fig:cons_protocol_2}, the receiver is responsible for forwarding each received message to its associated shadow. 
Instead of having two parallel replica groups in which main communicates with main and shadow communicates with shadow~\cite{ferreira_sc_2011}, we choose to let the mains forward each message to the shadows. This allows us to speed up a single lagging shadow when a main fails. 
At the same time, the shadows are suppressed from sending out messages. 
We assume that two copies of the same message are sent in an atomic manner.

\begin{figure}[!h]
  \begin{center}
  	\subfigure[Sender-forwarding protocol.]
		{
			\label{fig:cons_protocol_1}
      		\includegraphics[width=0.45\columnwidth]{Figures/cons_protocol_1}
      	}
  	\subfigure[Receiver-forwarding protocol.]
		{
			\label{fig:cons_protocol_2}
      		\includegraphics[width=0.45\columnwidth]{Figures/cons_protocol_2}
      	}
  \end{center}
  \caption{Consistency protocols for rsMPI.}
  \label{fig:cons_protocol}
\end{figure}



For MPI point-to-point communication routines, we add wrappers to implement the above consistency protocols. For sending functions, such as MPI\_Send() and MPI\_Isend(), the sender-forwarding protocol requires the main to duplicate the sending, while the receiver-forwarding protocol just does the normal sending. For receiving functions, such as MPI\_Recv() and MPI\_Irecv(), the sender-forwarding protocol requires that both the main and the shadow does one receiving from the main process at the sending side, while the receiver-forwarding protocol changes the main to perform a sending following a receiving and changes the shadow to receive from its own main. 
Collective communication in rsMPI uses point-to-point communication in a binomial tree topology, which demonstrates excellent scalability.

Assuming that only MPI operations can introduce non-determinism, the SYNC message shown in Figure~\ref{fig:cons_protocol} is used to enforce consistency when necessary. For example, under the sender-forwarding protocol MPI\_ANY\_SOURCE receiving may result in different message orders between a main and its shadow. To address this issue, we serialize the receiving of MPI\_ANY\_SOURCE messages by having the main finish the receiving and then use a SYNC message to forward the message source to its shadow, which then performs a normal receiving from the specific source. Other operations, such as MPI\_Wtime() and MPI\_Probe(), require both protocols to use the SYNC messages. Similar to MPI\_ANY\_SOURCE receiving, they are dealt with by forwarding each result from a main to its associated shadow.

\begin{table}[!h]
\caption{Comparison between sender-forwarding protocol and receiver-forwarding protocol. $N$ is the number of application message. $D$ is the number of non-deterministic event. $P$ is the number of process.}
\centering
\begin{tabular}{|c | c | c |}
\hline 
 & Sender-forwarding  & Receiver-forwarding  \\
\hline \hline 
Number of application message  & $2N$ & $2N$ \\
\hline
Number of SYNC message & $D$ &  $\le D$ \\
\hline
Shadow blocks main & Possible & Possible \\
\hline
Number of socket connection & $P^2$ & $P$ \\
\hline
Communication optimization & Hard & Easy \\
\hline
\end{tabular}
\label{tbl:cons_protocol_comp}
\end{table}

From the above analysis, we already see differences between the sender-forwarding protocol and the receiver-forwarding protocol. When deployed, they are expected to have further disparity on the cost and performance. 
A more detailed comparison is given in Table~\ref{tbl:cons_protocol_comp}. If an MPI application sends $N$ messages, both protocols double the number of messages to be sent. Also, in both protocols, a slower shadow may block a faster main when message is forwarded from the main to the shadow. Therefore, flow control and buffer management need to be considered (to be discussed in following section). As mentioned above, the receiver-forwarding protocol does not need SYNC message in the case of MPI\_ANY\_SOURCE receiving, thus requiring fewer SYNC messages than the number of non-deterministic event, $D$. Furthermore, assuming an all-to-all connection of $P$ processes, the sender-forwarding protocol implies $P^2$ socket connections between mains and shadows, while the receiver-forwarding protocol only requires $P$ connections. Lastly, since each shadow only communicates with its associated main under the receiver-forwarding protocol, it is easier to optimize communication via process to processor mapping. For example, a heuristic may be that each shadow is placed in the same rack as its main, but not on the same node. Given the advantages of the receiver-forwarding protocol, we focus on this protocol in the following discussion and performance evaluation.



\section{Coordination between main and shadow}
Each shadowed set has a coordinator process dedicated to coordination between the mains and shadows in the set. 
Coordinators do not execute application code, but just wait for rsMPI defined control messages, and then carry out the required 
coordination work accordingly. There are three types of control messages: termination, failure, and leaping. They corresponds to three actions:
\begin{itemize}
  \item When a main process finishes, the coordinator forces the associated shadow process to terminate immediately.
  \item When a main process fails, the coordinator speeds up the associated shadow by temporarily suspending the other collocated shadows, until the recovery is complete.
  \item When a main process initiates a failure-induced leaping, the coordinator triggers leaping at the associated shadow process.
\end{itemize}


To minimize resource usage, each coordinator is collocated with the shadows in the shadowed set. 
A coordinator performs  minimal work, as its main task is to simply handle incoming control messages.  As such, the impact of the coordinator on the execution rate of the collocated shadows is minimal. To separate control messages from data messages, rsMPI uses a dedicated MPI communicator for the control messages. This Control Communicator is created by the wrapper of the MPI\_Init() function. In addition, to ensure fast response and minimize the number of messages, coordinators also use OS signals to communicate with their collocated shadows. This is illustrated in Figure~\ref{fig:coordinator}, assuming a collocation ratio of 2.


\begin{figure}[!t]
  \begin{center}
      \includegraphics[width=\columnwidth]{Figures/coordinator}
  \end{center}
  \caption{A coordinator is added to each shadowed set. In this example, collocation ratio is 2 and each shadowed set contains 2 mains and 2 shadows.}
  \label{fig:coordinator}
\end{figure}



\section{Flow control}
As discussed in Section~\ref{sec:msg_consistency}, rsMPI requires that mains forward application messages to shadows. Since the shadows are scheduled to execute at a slower rate than the mains, an implication is that a shadow may block a main when a message is forwarded from the main to the shadow. As a consequence, this may slow down the mains to proceed at the same rate as the shadows, significantly hurting the performance. Due to message forwarding, another issue is that divergence will cause messages to accumulate at the shadow side, eventually leading to buffer overflow. To simultaneously addressing these two issues, srMPI adopts a layered architecture, as shown in Figure~\ref{fig:flow_control}, to establish communication channels between mains and shadows with flow control capability. Under the receiver-forwarding protocol, each pair of main and shadow only requires one such channel. 

\begin{figure}[!t]
  \begin{center}
      \includegraphics[width=\columnwidth]{Figures/flow_control}
  \end{center}
  \caption{A layered architecture for flow control in srMPI.}
  \label{fig:flow_control}
\end{figure}

At the lowest level, a socket connection to transfer application messages is established between each main and its associated shadow. When Remote Direct Memory Access (RDMA) is available, such as on the InfiniBand interconnects, srMPI uses the rsocket library, which provides a socket programming interface but internally uses RDMA for communication. Rather than copying data to the buffers of the operating system, RDMA enables the network adapter to transfer data directly from the main process to its shadow. The zero-copy networking feature of RDMA considerably reduces latency, thereby enabling fast transfer of data. When RDMA is not available, srMPI reverts back to TCP connections. 


At each shadow, a circular message queue, referred to as MQ, is implemented to store the messages that have been sent by main but not yet consumed by shadow. Similar to Linux socket buffer or Unix mbuf, MQ consists of two layers. The control layer contains fixed-length elements, each of which stores the meta-data of a message. The data layer is the actual buffer where variable-length message contents are stored. The control layer and data layer are connected via pointers. 

At the process level, a helper thread is added to each main and shadow process. The shadow helper thread is responsible for quickly responding to messages sent by the main on the socket connection, thus avoiding blocking the main. Every time there is a message, the shadow helper thread calls socket recv() to receive the message, and then store it in its MQ. The main helper thread is mainly used to assist buffer-forced leaping. To avoid buffer overflow, a threshold is specified for MQ. If the threshold is reached, the shadow helper thread initiates a buffer-forced leaping, and notifies the corresponding main helper thread to participate in the leaping at the same time. To minimize the impact of the helper threads on the performance of the compute threads, each helper thread is forced to relinquish CPU until a message arrives.





\section{Leaping}
Checkpoint/restart requires each process to save its execution state, which can be used later to retrieve the state of the computation. 
Leaping is similar to the checkpointing process, except that the state is directly transferred between a pair of main and shadow, thus requiring no additional storage space. 
To reduce the size of data involved in saving state, rsMPI uses a similar approach as application-level checkpointing~\cite{beguelin1997application,ni_2013_acr}, and requires users to identify necessary data as process state using the following API:

\begin{lstlisting}
void  leap_register_state(void *addr, int count, MPI_Datatype dt);
\end{lstlisting}

For each data set to be registered, three arguments are needed: a pointer to the memory address of the data set, the number of data items in the data set, and the datatype. 
Application developer could use domain knowledge to identify only necessary state data, or use compiler techniques to automate this process~\cite{Bronevetsky:09:Compiler}. 
Internally, rsMPI uses a linked list to keep track of all registered data. After each call of leap\_register\_state(), rsMPI adds a node to its internal linked list to record the three parameters. 
During leaping, the linked list is traversed to retrieve all registered data as the process state.

MPI communication channels are used in rsMPI to transfer process state. Although multiple data sets can be registered as a process' state, only a single message needs to be transferred, as MPI supports derived datatypes. To isolate state messages from application messages, rsMPI uses the Control Communicator to transfer process state.  
By using a coordinator to synchronize the leaping and relying on  MPI messages to rapidly transfer state, the overhead of leaping is minimized. 

To make sure a pair of main and shadow stay consistent after a leaping, not only user-defined states should be transferred, but also lower level states, such as program counter and message buffers, need to be correctly updated. Specifically, the leap-recipient needs to satisfy two requirements:  
1) Discard all obsolete messages after the leaping; 2) Resume execution at the same point as the leap-provider. We discuss our solutions below, under the assumption that the application's main body consists of a loop of iterations, which is true in most HPC applications.

To correctly discard all obsolete messages without throwing away useful ones, rsMPI requires every main and shadow maintain a counter for the messages consumed. Then during leaping, the counter value at the leap-provider is transferred to the leap-recipient, so that the latter knows how many messages to discard.
To resume execution from the same point, we restrict leaping to always occur at specific points, and use an internal counter to make sure that both the leap-recipient and leap-provider start leaping from the same point. For example, when a main initiates a leaping, the coordinator will trigger a rsMPI-defined signal handler at the associated shadow. The signal handler does not carry out leaping, but sets a flag for leaping and receives from its main a counter value that indicates the leaping point. %Then, the shadow will check the flag and compare the counter value at every possible leaping point. 
Only when both the flag is set and counter value matches will the shadow start leaping. In this way, it is guaranteed that after leaping the leap-recipient and leap-provider will resume execution from the same point. To balance the trade-off between implementation overhead and flexibility, we choose MPI receive operations as the only possible leaping points. 

\section{Evaluation}
We deployed rsMPI on a cluster of 30 nodes (600 cores) for testing and benchmarking. Each node consists of a 2-way SMPs with Intel Haswell E5-2660 v3 processors of 10 cores per socket (20 cores per node). Each node is configured with 128 GB of local memory. Nodes are connected via 56 GB/s FDR InfiniBand. To maximize the compute capacity, we used up to 20 cores per node.

Benchmarks from the Sandia National Lab Mantevo Project and NAS Parallel Benchmarks (NPB) are used, and evaluated rsMPI with various problem sizes and number of processes. 
CoMD is a proxy for molecular dynamics application. MiniAero is an explicit unstructured finite volume code that solves the Navier-Stokes equations. Both MiniFE and HPCCG are unstructured implicit finite element codes, but HPCCG uses MPI\_ANY\_SOURCE receive operations and can demonstrate rsMPI's capability of handling non-deterministic events. IS, EP, and CG from NPB represent integer sort, embarrassingly parallel, and conjugate gradient applications, respectively. These applications cover key simulation workloads and represent both different communication patterns and computation-to-communication ratios.

We also implemented checkpoint/restart to compare with rsMPI in the presence of failures. 
To be optimistic, we chose double in-memory checkpointing that is much more scalable than disk-based checkpointing~\cite{zheng_2004_ftccharm}. 
Same as leaping in rsMPI, our application-level checkpointing provides an API for process state registration. This API requires the same parameters, but internally, it allocates extra memory in order to store the state of a ``buddy" process. Another provided API is checkpoint(), which inserts a checkpoint in the application code. For fairness, MPI messages are used to transfer state between buddies.  
For both rsMPI and checkpoint/restart, we assume a 60 seconds rebooting time after a failure. All figures in this section show the average of 5 runs with the standard deviation.

\subsection{Measurement of runtime overhead}
\label{sec:runtime_overhead}
While the hardware overhead for rsMPI is straightforward (e.g., collocation ratio of 4 results in the need for 25\% more hardware cost), 
the runtime overhead due to the enforced consistency protocol and coordination depend on applications. To measure this overhead we ran each benchmark application linked to rsMPI and compared the execution time with the baseline, in which each application runs with unmodified OpenMPI.

Figure~\ref{fig:runtime_overhead} shows the comparison of the execution time for the 7 applications in the absence of faults. All the experiments are conducted with 256 application-visible processes. That is, the baseline uses 256 MPI ranks, while rsMPI uses 256 mains together with 256 shadows. The baseline execution time varies from seconds to half an hour, so we plotted the time in log-scale. 
From the figure we can see that rsMPI has comparable execution time to the baseline for all applications except IS. The reason for the exception is that IS uses all-to-all communication and is heavily communication-intensive. This is verified by adding fake computation to the application and we can see an immediate drop of the overhead to negligible level. 
We argue that communication-intensive applications like IS are not scalable, and as a result, they are not suitable for large-scale HPC. 
For all other applications, the overhead varies from 0.64\% (EP) to 2.47\% (CoMD). Even for HPCCG, which uses MPI\_ANY\_SOURCE, the overhead is only 1.95\%, thanks to the flow control mechanism deployed in srMPI. 
Therefore, we conclude that rsMPI's runtime overheads are modest for applications that exhibit a fair communication-to-computation ratio.

\begin{figure}[!t]
  \begin{center}
      \includegraphics[width=0.6\columnwidth]{Figures/runtime_overhead_hpcc}
  \end{center}
  \caption{Comparison of execution time between baseline and rsMPI using 256 application-visible processes and collocation ratio of 2 for rsMPI.}
  \label{fig:runtime_overhead}
\end{figure}

\subsection{Scalability}
%In addition to measuring the runtime overhead at fixed application-visible process count, we also assessed both strong and weak scalability by varying the number of processes for the applications. Strong scaling is defined as how the execution time varies with the number of processes for a fixed total problem size. In contrast, weak scaling is defined as how the execution time varies with the number of processes for a fixed problem size per process.

In addition to measuring the runtime overhead at fixed process count, we also assessed the applications' weak scalability, which measures how the execution time varies with the number of processes for a fixed problem size per process.
Among the seven applications, HPCCG, CoMD, and miniAero allow us to configure the input for weak scaling test. The results for miniAero are similar to those of CoMD, so we only show the results for HPCCG and CoMD in Figure~\ref{fig:scalability}. %Figure~\ref{fig:scalability} reveals that both HPCCG and CoMD have good strong scalability. By increasing the number of processes, we can always reduce the execution time for a fixed problem size. At the same time, rsMPI's runtime overhead increases with the number of processes during the strong scalability test. At 256 processes, the overhead reaches 13.2\% for CoMD, and 29.1\% for HPCCG. This may seem to contradict with the results in Section~\ref{sec:runtime_overhead}. It is expected, however, since increasing the number of processes while keeping a constant problem size increases the communication-to-computation ratio of the application. Hence, to keep rsMPI overheads reasonable, it is important to choose input sizes such that the ratio of communication-to-computation is balanced. 


\begin{figure}[!t]
	\begin{center}
		\subfigure[HPCCG weak scalability]
		{
			\label{fig:hpccg_weak}
			\includegraphics[width=0.45\columnwidth]{Figures/hpccg_weak_hpcc}
		}
		\subfigure[CoMD weak scalability]
		{
			\label{fig:comd_weak}
			\includegraphics[width=0.45\columnwidth]{Figures/comd_weak_hpcc}
		}
	\end{center}
	\caption{Scalability test for number of processes from 1 to 256. Collocation ratio is 2 for rsMPI.}
	\label{fig:scalability}
\end{figure}

Comparing between Figure~\ref{fig:hpccg_weak} and Figure~\ref{fig:comd_weak}, it is obvious that HPCCG and CoMD have different weak scaling characteristics. While the execution time for CoMD increases by 8.9\% from 1 process to 256 processes, the execution time is almost doubled for HPCCG. However, further analysis shows that from 16 to 256 processes, the execution time increases by only 2.5\% for CoMD, and 1.0\% for HPCCG. We suspect that the results are not only affected by the scalability of the application, but also impacted by other factors, such as cache and memory contention on the same node, and network interference from other jobs running on the cluster. Note that each node in the cluster has 20 cores and we always use all the cores of a node before adding another node. Therefore, it is very likely that the node level contention leads to the substantial increase in execution time for HPCCG. The results from 16 to 256 processes show that both HPCCG and CoMD are weak scaling applications. 
%To predict the overhead at exascale, we applied curve fitting to derive the correlation between runtime overhead and the number of processes. At $2^{20}$ processes, it is projected that the overhead is 3.1\% for CoMD and 7.6\% for HPCCG. 

Similar to the results of the previous section, the runtime overhead for rsMPI is modest. The maximum overhead observed is 5.0\% when running CoMD with 32 processes. Excluding this case, the overhead is always below 2.1\%. To predict the overhead at exascale, we applied curve fitting to derive the correlation between runtime overhead and the number of processes. At $2^{20}$ processes, it is projected that the overhead is 3.1\% for CoMD and 7.6\% for HPCCG. 


\subsection{Performance under failures}
%The last set of experiments test rsMPI's capability of tolerating failures and evaluate its performance under various failures by comparing with checkpointing/restart. 

As one main goal of this work is to achieve fault tolerance, an integrated fault injector is required to evaluate the effectiveness and efficiency of rsMPI to tolerate failures during execution. 
To produce failures in a manner similar to naturally occurring process failures, the failure injector is designed to be distributed and co-exist with all rsMPI processes. Failure is injected by sending a specific signal to a randomly picked target process.

We assume that the underlying hardware platform has a Reliability, Availability and Serviceability (RAS) system that provides failure detection. In our test system, we emulate the RAS functionality by associating a signal handler with every process. The signal handler catches failure signals sent from the failure injector, and uses a rsMPI defined failure message via a dedicated communicator to notify all other processes of the failure\footnote{This functionality has been tested in \cite{fang2017letgo}. By using GDB to intercept OS signals generated after a fault, a signal handler allows the failing process to continue work before crashing.}. 
To detect failure of another process, rsMPI receiving operation checks for failure messages before performing the actual receiving. 
Similar to ULFM~\cite{bland2013post}, a process in rsMPI can only detect failure when it posts an MPI receive operation. 

The first step was to test the effectiveness of leaping.  
Figure~\ref{fig:single_failure} shows the execution time of HPCCG with a single failure injected at a specific time, measured as a proportion of the total execution of the application, at an increment of 10\%.
The execution time is normalized to that of the failure-free baseline.   
The blue solid line and red dashed line represent rsMPI with collocation ratio of 2 and 4, respectively. For simplicity, they are referred to as rsMPI\_2 and rsMPI\_4 in the following text.  


\begin{figure}[!t]
  \begin{center}
      \includegraphics[width=0.6\columnwidth]{Figures/single_failure_hpcc}
  \end{center}
  \vskip -0.2in
  \caption{Execution time of HPCCG under rsMPI with a single failure injected at various time, normalized to that of the failure-free baseline.}
  \label{fig:single_failure}
\end{figure}

As shown in Figure~\ref{fig:single_failure}, rsMPI's execution time increases with the failure occurrence time, regardless of the collocation ratio. The reason is that recovery time in rsMPI is proportional to the amount of divergence between mains and shadows, which grows with the execution. 
Another factor that determines the divergence is the shadow's execution rate. The slower the shadows execute, the faster the divergence grows. As a result, rsMPI\_2 can recover faster than rsMPI\_4, and therefore achieves better execution time.



The results in Figure~\ref{fig:single_failure} suggests that rsMPI is better suited to environments where failures are frequent. 
This stems from the fact that, due to leaping, the divergence between mains and shadows is eliminated after every failure recovery. As the number of failure increases, the interval between failures decreases, thereby reducing the recovery time per failure.
To demonstrate the above analysis, we compare rsMPI with checkpoint/restart under various failure rates. 
To run the same number of application-visible processes, rsMPI needs more nodes than checkpointing to host the shadow processes. 
For fairness, we take into account the extra hardware cost for rsMPI by defining the weighted execution time:
$$T_{weight} = T_e \times S_p,$$ where $T_e$ is the wall-clock execution time and $S_p$ is the projected speedup. For example, we measured that the speedup of HPCCG from 128 processes to 256 processes is 1.88, and rsMPI\_2 needs 1.5 times more nodes than checkpointing, so the projected speedup is $1.5\times\frac{1.88}{2}=1.41$. Similarly, we calculate the projected speedup for rsMPI\_4 as $1.25\times\frac{1.88}{2}=1.17$.


%The buffer-forced leaping interval for an application is selected such that no buffer overflow at the shadows would take place. Therefore, the interval should vary from system to system and also depends on the application communication patterns. We assume checkpoint/restart has the same buffer pressure as it needs to perform message logging, so its checkpointing interval is selected based on the same metric as rsMPI. We evaluated rsMPI with 2 different collocation ratios, i.e., 2 and 4. When collocation ratio is 2, rsMPI uses 50\% more nodes than checkpoint/restart, and the execution rate of each shadow is roughly 50\% of the processor rate. Therefore, we set the checkpointing interval to be the same as the forced leaping interval for rsMPI. When collocation ratio is 4, rsMPI needs 25\% more nodes, and each shadow's rate is roughly 25\% of the processor rate. As a result, we loose the checkpointing interval to be twice of the forced leaping interval. 


In this analysis, we set the checkpointing interval to $0.1T$, where $T$ is the total execution time. To  both checkpoint/restart and rsMPI, we randomly inject over $T$ a number of faults, $K$, ranging from 5 to 30. This fault rate corresponds to a processor's MTBF of $NT/K$, where $N$ is the number of processors. That is, the processor's MTBF is proportional to the total execution time and the number of processors. For example, when using $256$ processors and executing for $1700$ seconds, injecting $10$ faults corresponds to a processor's MTBF of $12$ hours. However, 
when using a system of $64,000$ processors and executing over $4$ hours, injecting $10$ faults corresponds to a processor's MTBF of $3$ years.

Figure~\ref{fig:multiple_failure} compares checkpoint/restart and rsMPI, based on both wall-clock and weighted execution time. Ignoring the hardware overhead, Figure~\ref{fig:failures_time} shows that, when the number of failures is small (e.g., 5 failures), checkpoint/restart slightly outperforms rsMPI. As the number of failures increases, however, rsMPI achieves significantly higher performance than checkpoint/restart. For example, when the number of failures is 20, rsMPI\_2 saves 28.7\% in time compared to checkpoint/restart. The saving rises up to 39.3\%, when the number of failures is increased to $30$. Compared to checkpoint/restart, rsMPI\_4 reduces the execution time by 19.7\% and 34.8\%, when the number of failures are 20 and 30, respectively. 

\begin{figure}[!t]
  \begin{center}
      \subfigure[Wall-clock execution time]
		{
			\label{fig:failures_time}
			\includegraphics[width=0.45\columnwidth]{Figures/failures_time_hpcc}
		}
		\subfigure[Weighted execution time]
		{
			\label{fig:failures_ntime}
			\includegraphics[width=0.45\columnwidth]{Figures/failures_ntime_hpcc}
		}
  \end{center}
  \caption{Comparison between checkpoint/restart and rsMPI with various number of failures injected to HPCCG. 256 application-visible processes, 10\% checkpointing interval.}
  \label{fig:multiple_failure}
\end{figure}


Careful analysis of Figure~\ref{fig:failures_time} reveals that, as the number of failures increases, checkpoint/restart and rsMPI exhibit different performance degradation. As expected, the execution time for checkpoint/restart increases proportionally with the number of failures. For rsMPI, however, the increase is sub-linear. This is due to fact that as more failures occur, the interval between failures is reduced, and as a result, the recovery time per failure is also reduced. Although not shown in Figure~\ref{fig:failures_time}, rebooting time will eventually dominate the recovery time when more failures occur, resulting in a linear increase in execution time for rsMPI. This increase, however, occurs at a significantly slower rate than checkpoint/restart.

Incorporating hardware overhead, Figure~\ref{fig:failures_ntime} compares the weighted execution time between checkpoint/restart and rsMPI.  As expected, checkpoint/restart is better when the number of  failures is small (e.g., 5 failures).  When the number of failures increases,  however, checkpoint/restart loses its advantage quickly. At 30 failures, for example, rsMPI\_2 and rsMPI\_4 are 19.3\% and 31.3\% more efficient than checkpoint/restart, respectively.
Note that, when comparing rsMPI\_2 and rsMPI\_4, the former shows higher performance with respect to wall-clock execution time, while the latter is better with respect to weighted execution time. 

\section{Summary}
Implementation details. Thanks to the design of Leaping Shadows, rsMPI is able to tolerate multiple failures.

Building blocks.

Empirical experiments demonstrated that the Rejuvenating Shadows model outperforms in-memory checkpointing/restart in both execution time and resource utilization, especially in failure-prone environments.




