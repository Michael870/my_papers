In the previous section we explained why naive resource allocation in virtual environments (Figure~\ref{fig:static_allo})
is sub-optimal for HPC workloads. This section will introduce our novel design that incorporates 
CPU and memory over-commitment for resource optimization. First, we start by introducing 
vHERO with CPU over-commitment. Then, we continue to discuss the 
inclusion of memory over-commitment which is much more challenging. Lastly, we complete the design 
with dynamic VM migration, which mitigates potential 
performance penalties from resource over-commitment. 

\subsection{vHERO with CPU over-commitment}
With a share-based CPU scheduler in the hypervisor, the allocation of CPU resource among VMs on a 
shared host does not entirely rely on the number of vCPUs, but also is affected by the configurable shares. 
This offers system admins the freedom to 
change VM vCPU count without worrying about impacting resource allocation. vHERO takes advantage of this 
flexibility and configures each tenant with the same amount of virtual CPUs as the physical CPUs 
on a node. In this way, each tenant can get access to all the CPUs inside the node, and consequently, if one tenant 
is not consuming its entire quota, the free CPU cycles can be used by other tenant(s). 
This approach can be easily scaled to a cluster by repeating the configuration on every 
single node. Then, each tenant is allocated a virtual cluster that consists of one VM per node. From the end user 
perspective, each tenant gets exclusive access to a dedicated cluster with security and fault isolation, and even better performance.  
Using the previous example of sharing four nodes between two tenants,  
vHERO is illustrated in Figure~\ref{fig:vtc}. In the experiment section later, we will demonstrate  
support of up to four tenants simultaneously on a single physical cluster with 4X CPU over-commitment.

\begin{figure}[!t]
   \begin{center}
       \includegraphics[width=\columnwidth]{Figures/allocation3}
   \end{center}
   \caption{Illustration of vHERO on four nodes for two tenants.}
   \label{fig:vtc}
 \end{figure}

\subsection{Adding memory over-commitment}
Similar to CPU over-commitment, memory can be configured in a way that the combined VM memory can be 
larger than the physical memory capacity. It is impossible, however, that all VMs actively access all configured 
memory beyond the physical capacity. This is mainly due to performance considerations. TPS, ballooning, and 
compression all have no guarantee to reclaim memory in a timely manner, and if all these techniques fail, 
hypervisor swapping will occur to swap out guest OS memory to disk, rendering unacceptable performance to 
the HPC workloads. Two questions that we address through our performance study are: 1) whether memory over-commitment 
can be practical; 2) how far can we go with active memory usage under memory over-commitment.

\subsection{Dynamic VM migration}
It's very restricting that while one can configure more VM memory, the VMs are not able to consume all of it 
at the same time. This constraint applies to every node in the cluster because if memory is over stressed on any 
node, the progress of the whole workload is impacted. 
Fortunately, vMotion can be applied to relax the constraint~\cite{infrastructure2006resource}. 
It is often the case that nodes in an HPC cluster have varying memory load~\cite{gupta2013improving}. While some nodes have memory contention 
between VMs, other nodes may have a decent amount of free memory. In such cases, vHERO with memory over-commitment 
is much more promising because VMs can be dynamically migrated based on load changes to  
spread the memory load evenly across the physical cluster. Ideally, none of the nodes will experience sustained memory pressure as long as 
the total active memory does not exceed the physical cluster memory capacity. This essentially relaxes the previous 
per-node constraint to a constraint per cluster, which has much more room for memory pressure tolerance. 